\documentclass[12pt]{article}

\input{header/macros.tex}
\input{header/packages.tex}

\title{Adapting to Quasi-Periodic and Secular Changes of Climate Risk}
\author[1,2]{James Doss-Gollin}
\author[1,2]{David Farnham}
\author[1,2]{Upmanu Lall}
\affil[1]{Columbia Water Center, Columbia University}
\affil[2]{Department of Earth and Environmental Engineering, Columbia University}

\input{header/glossaries.tex}
\input{header/biblatex.tex}
\addbibresource{library.bib}
\graphicspath{{../figs/}}

% -----------------------------------------------------------------------------
% AUTHOR AND TITLE AND TITLEPAGE
% -----------------------------------------------------------------------------

\begin{document}
\maketitle
\RaggedRight{}
\begin{abstract}
  This is a first conceptual step to the development of a full sequential decision analysis approach under nonstationarity.
\end{abstract}

% -----------------------------------------------------------------------------
% INTRODUCTION
% -----------------------------------------------------------------------------

\section{Introduction}\label{sec:introduction}

The \nth{20} century saw the evolution of a nearly universal approach to the statistical analysis of climate extremes for use in risk analysis and mitigation studies. 
In this framework, event risk is assessed as a relationship between the event magnitude and its annual probability of exceedance.
Under the assumption that the historical or simulated record of events constitutes an stationary, \gls{iid} process, and using an appropriate statistical inference model, an estimate of this relationship and often its uncertainty based on a finite $N$-year record is developed.
This is then used as a measure for decision analysis for infrastructure, flood zoning and insurance instruments for event risk mitigation, typically using expected net present value analyses \citep[\ie{}][]{Rosner2014}, applied to the costs and residual losses expected from the mitigation instrument.
The uncertainty associated with the model may be used to develop a risk premium for insurance, or for assessing the potential over- or under- design for infrastructure.

Over time, it has become clear that even small fluctuations in the dynamics of the hydroclimate system, typically called nonstationarity, can lead to clustering of extreme events  and strong time dependence of a public or private actor's risk profile \citep{Bonnafous2017a,Bonnafous2017b}.
For example, in 2011 four typhoons struck northern Thailand \citep{Gale2013}, causing severe damage to public and private infrastructure and crippling global supply chains \citep{Haraguchi2015}.
Similarly, in June 2013 three summertime cyclones followed highly similar tracks, leading to widespread flooding in central Europe \citep{Grams2014}.
More recently the 2017 Atlantic hurricane season led to over \usd{130 billion} in insured losses \citep{SwissRe2017}, despite negligible losses in the preceding $N$ years.

In recent years a plethora of efforts have aimed at identifying the nature of the nonstationarity and modeling it, \eg{} by introducing time trends on statistical parameters \citep{Strupczewski2001,Vogel2011,Obeysekera2014,Serinaldi2015}, climate covariates \citep{Delgado2014,Griffis2007,Silva2016,Sun2014,Hall2014}, latent variable or state-based modeling approaches \citep{Waylen1986,Sveinsson2005,Griffis2007}, or using only the last $N$ years of the data.
Despite such efforts, how one should use these nonstationarity estimates for the design of risk mitigation infrastructure, or the pricing of insurance like instruments has been much less studied.

Two forms of nonstationarity are of concern.
The first, secular change, may be related to anthropogenic climate change, river modification, or land use change \citep{Milly2008,Merz2014}.
The uncertainties associated with this are not purely informational, at least in the sense that they can be inferred from a historical sample or model.
Both nature and human actions over the course of this century will determine the magnitude and direction of secular change, and neither is fully predictable.
The approach being taken by many, including the \gls{ipcc}, is to consider discrete scenarios for what humans may do with respect to greenhouse gas emissions, and then attempt to use climate models to estimate the probability of extreme events conditional on occurrence of a particular scenario.
Because of inherent errors due to the discretization and parameterization of the equations of motion, as well as uncertainty about the underlying physical parameters, bias correction methods \citep[\eg quantile-quantile mapping;][]{Rajczak2016} are typically applied to the final outputs.
While this typically improves the model chain's representation of the historical record, this approach assumes a stationary relationship between modeled and observed rainfall at a point-by-point scale, an assumption that is unlikely to hold under the kinds of spatial and dynamical shifts anticipated under increased greenhouse gas scenarios such as expansion of the tropics and altered equator-pole temperature contrasts.\citep{Dankers2009,Ott2013,Merz2014,Dittes2017}.\footnote{Farnham et al 2018!}

% This is a strange paragraph,.
A critical question in this regard is how to assess whether any structure designed using these projections may actually perform in the future relative to the proposed level of risk protection.
Would it be over- or under- designed?
Would the high degree of bias and uncertainty in its estimation lead to suboptimal investment strategies in risk mitigation?
These questions go well beyond what can be reliably answered at this time.
This kind of epistemic uncertainty clearly grows as we look out into the distant future, and the importance of investment decisions that may engage us in considerations of designs that go far out into the future, e.g., 2100AD, diminish if any positive discount rate is used for the future.
While this consideration offsets the impact of the growing uncertainty for longer project life, it also suggests that it may be more effective to understand the consequences for analysis that may focus on shorter lived mitigation alternatives, ones that may last for 1, 5, 10, 20,  30 or 50 years.
Sequences of such projects may better reflect climate adaptation goals in any case, especially if we believe that major climate changes may only happen beyond these horizons, or if we are optimistic and believe that humans may avert much of the potential catastrophe by the end of the longer of these horizons.

Focusing on these shorter time horizons brings up the concern associated with the second type of nonstationarity in climate: the quasi-periodic inter-annual, decadal and multidecadal modes of climate variability \citep[see][for a review]{Hannachi2017} that influence the spatiotemporal expression of climate extremes in ways that violate the assumption of \gls{iid} draws of events that is central to classical analyses \citep{Matalas2012}.
For example, \citet{Jain2001} considered how \gls{enso}, an example of one such process, impacts flood risk when there is zero secular change.
Using a \SI{1000}{year} synthetic \gls{enso} time series, the largest event from a block of $N$ years was sampled from the historical record as the target design level.
Next, the number of times this magnitude was exceeded in the subsequent block of $N$ years was then recorded.
A corresponding randomized experiment was also conducted where the $N$ years in each step were selected at random, consistent with an \gls{iid} process hypothesis.
Crucially, the bootstrap frequency distribution associated with event magnitude exceedance process for the structured blocks was dramatically fatter tailed than the one for the randomized blocks, demonstrating that extreme events are significantly clustered in time relative to an \gls{iid} process.
This finding also held when a long sequence of historical streamflow observations was used in place of synthetic data.

Given the \gls{iid} assumptions in her analysis of climate extremes, a typical risk analyst uses annualized cost-benefit analyses over the projected life of a structural risk mitigation project, and the expected loss in a given year from the exceedance of the nominal design level is computed as the potential loss* probability of exceedance. Clearly, in a nonstationary world, the probability of exceedance could be considered to be temporally variable over the project life. This would however, need to be a prediction of that probability ex ante, using appropriate modeling tools applied to the historical data or to climate model based forecasts. Faced with nonstationarity, some analysts propose using only the most recent subset of the data, e.g., the last 20 or 30 years to build the event magnitude – probability of exceedance relationship. This can be thought of as a “persistence” model, which may be “better than nothing”, and may or may not include a trend in the parameters of the probability distribution. How many years should one retain from the recent past? Too few and the estimation uncertainty dominates. Too many and the bias may dominate. Of course, if the underlying climate is quasi-periodic then this choice can be arbitrarily poor, since the last batch of K years may be of exactly the opposite phase of the oscillation as what comes next. In summary, we need to carefully consider the number of years of historical data, N, and the number of future years M over which the instrument of interest will be evaluated, as well as efforts at using the data from the N year period to predict what will happen in the M years. The risk estimation from the N years needs to explicitly inform a risk prediction over the M years, and depending on the approach and the form of the underlying nonstationarity, we can expect both bias and uncertainty associated with the prediction, and we need temporally varying probabilities to address the inevitable risk clustering. This is then the statement of the risk mitigation design problem that we need to consider for nonstationary climate. This paper is the first to formally develop these ideas in the context of a decision analytic framework.

We focus on structured experiments with synthetic and actual data on extremes to illustrate how one could approach the analyses and what the implications are of varying N and M on the ability to do effective inference and on the ability to judge operational performance. We consider idealized cases for the structural and financial instruments to convey the ideas, and defer more complex sequential decision making tasks to a subsequent paper. The idea that one needs to consider the joint uncertainty in the estimation and occurrence of climate extremes for the N and M periods for stationary conditions was originally considered by Lall (198x). That analysis corresponds to what one would expect under i.i.d. conditions, and would hence provide a baseline to compare the uncertainty distribution associated with a portfolio whose elements were designed using N years of data with an application to M future years at a common design probability of exceedance for the elements. 


\section{Introduction}

In 2011 four typhoons struck northern Thailand \citep{Gale2013}, causing severe damage to public and private infrastructure and crippling global supply chains \citep{Haraguchi2015}.
In June 2013, a rainy spring was followed by three summertime cyclones following highly similar tracks, leading to widespread flooding in central Europe \citep{Grams2014}.
At the same time a multi-year drought in S\~ao Paolo threatened to dry out one of the world's largest cities \citep{Seth2015}, and more recently the 2017 Atlantic Hurricane season led to over \usd{130 billion} in insured losses \citep{SwissRe2017}.
These particular events suggest that core assumptions of classical analysis of climate extremes for use in risk analysis and mitigation studies, particularly that the historical or simulated record of events constitutes an independent and identically distributed process, are inconsistent with observations.

Indeed, the scientific literature has recognized that even small nonstationarity (temporal change in dynamics of the underlying physical process) can lead to significant trends in the estimated $T$-year return periods or probability of exceedance $p_T$ \citep{Milly2008,Vogel2011,Salas2014,Merz2014,Obeysekera2016}.\footnote{All of these citations are for floods, can we look at other types of extremes? Even droughts would be helpful.}
This has led to a plethora of efforts aimed at identifying the nature of the nonstationarity and modeling it, \eg{} by introducing time trends on statistical parameters \citep{Strupczewski2001,Vogel2011,Obeysekera2014,Serinaldi2015}, climate covariates \citep{Delgado2014,Griffis2007,Silva2016,Sun2014,Hall2014}, latent variable or state-based modeling approaches \citep{Griffis2007,Sveinsson2005,Waylen1986}, or using only the last $N$ years of the data.\footnote{Rich Vogel citation?}
Each model will necessarily give different estimates, which may fit the historical record with comparable accuracy but diverge at an increasing rate as the model is forecast further into the future.
Despite the wide suite of available models, \emph{the question of how one should use these nonstationarity estimates for the design of risk mitigation infrastructure or the pricing of insurance instruments has been much less studied}.

There are two kinds of nonstationarities of concern.
The first, secular change, may be related to anthropogenic climate change, river modification, or land use change \citep{Merz2014}.
The uncertainties associated with this are not purely informational; both nature and future human actions will determine the outcomes, and neither is obviously predictable.
The approach taken by many, including the IPCC, is to consider discrete scenarios for what humans may do with respect to greenhouse gas emissions and then use a sequence of physics-based numerical models with these scenarios to develop probabilistic statements as to the potential changes in the extremes.
Such a ``model chain'' typically encompasses:
\begin{enumerate*}[label= (\roman*) ]
  \item emission scenario;
  \item general circulation model (GCM);
  \item downscaling; and
  \item high-resolution model of the study region.
\end{enumerate*}
Because of inherent errors due to, \eg{} the discretization and parameterization of the equations of motion, bias correction methods are typically applied at each step of the chain; while this typically improves the model chain's representation of the historical record, the extrapolation of these bias corrections far into the future does not inspire confidence\footnote{Is "inspire confidence" too informal?... I'm not sure}. \citep{Dankers2009,Ott2013,Merz2014,Dittes2017}.\footnote{Farnham et al 2018!}
A critical question in this regard is how to assess whether any structure\footnote{or other risk management instrument such as an insurance product?} designed using these projections may actually perform in the future relative to the proposed level of risk protection.
Would it be over- or under- designed?
Would the bias and uncertainty in its estimation lead to sub-optimal investment strategies in risk mitigation?
These questions go far beyond what can be reliably answered at this time, and this type of epistemic uncertainty clearly grows as we look out into the distant future.\footnote{Perhaps these sentences belong elsewhere?}

In addition to secular change, quasi-periodic inter-annual to multi-decadal modes of climate variability \citep[see][]{Hannachi2017} to influence the spatiotemporal expression of climate extremes in ways that violate the assumption of independent, identically distributed events \citep[see][]{Merz2014,Serinaldi2015,Hoskins2015}.

When secular and quasi-periodic variability are both considered, it becomes clear the probability of exceedance is temporally variable over the life of a climate adaptation project.
In this paper we provide some initial insight into the bias and uncertainty associated with flood frequency estimation for projects as a function of the project planning period $M$ as well as the length of the historical record $N$.
This project planning period can be interpreted as the design life of a physical structure, or as the contract length of a financial instrument.
Our aim is not to identify the ``best'' model, but rather to develop theory as to how the underlying nature of the natural variability and nonstationarity and the choice of model used to model future flood risk map onto estimation bias and variance in the context of many different $M$ and $N$.
This estimation bias and variance may map directly onto the price of an insurance contract, or the possibility of over/under-design of a large structure like a dam or seawall used to protect against climate hazards.
Though the specific examples target flood risk, this method and framework may be used for understanding the implications of uncertain projections of the future for climate hazards including drought, hurricanes, and wind. \footnote{Overall comment on the introduction:  We may consider hinting at some of the results a bit more, otherwise the introduction is fairly wide open and I'm not sure the reader gets a good sense for what she is in for... but then again, perhaps that's OK}.

% -----------------------------------------------------------------------------
% METHODS
% -----------------------------------------------------------------------------

\section{Methods}\label{sec:methods}

We use a statistical framework with highly idealized models to generate sequences of annual-maximum floods, fit these sequences to probabilistic models that incorporate time in different ways, and evaluate the performance of these models for multiple observational record lengths $N$ and project planning periods $M$.

\subsection{Generating Synthetic Flood Sequences\label{sec:methods-generating}}

We generate synthetic flood sequences from two different simple conceptual models that represent chaotic dynamics and long memory.
For both cases, the expected value of the annual maximum flood follows a log-normal distribution, conditional on an expected value $\mu(t)$, which in turn depends on individual models described in \cref{sec:methods-nino,sec:methods-markov}:
\begin{equation} \label{eq:lognormal}
  \log Q(t) \big| \mu(t),\sigma(t) \sim \normal \qty(\mu(t), \sigma(t)).
\end{equation}
For simplicity we assume a constant coefficient of variation:
\begin{equation}\label{eq:calc-sigma}
  \sigma(t) = 
  \begin{cases} 
    \alpha \mu(t) & \qqtext{if} \alpha \mu(t) \geq \sigma_\text{min} \\ 
    \sigma_\text{min} & \qqtext{else}
  \end{cases}
\end{equation}
where $\sigma_\text{min}$ is a small constant that ensures non-trivial noise in the system.

The procedure to generate a sequence of streamflow is thus:
\begin{enumerate}
  \item Choose $t_0$, $M$, and $N$ so that the time is $\vb{t}=t_0-N+1, \ldots, t_0+M$.
  \item Use one of the models described below to simulate $\mu(t)$
  \item Use \cref{eq:calc-sigma} to calculate $\sigma(t)$
  \item For each time $t \in \vb{t}$, draw $Q(t)$ following \cref{eq:lognormal}.
\end{enumerate}
Though both approaches used to generate sequences $\mu(t)$ use highly simplified models neglecting many important mechanisms, they enable us to generate many synthetic sequences of $\qty[M + N]$ years which exhibit chaotic but organized quasi-periodic behavior (regime behavior or low-frequency variability) plus a trend term, thereby capturing some essential features of hydrological variability.

\subsubsection{Synthetic NINO3 Model\label{sec:methods-nino}}

ENSO is the dominant mode of tropical climate variability, impacts flood risk around the world, and varies in a chaotic, quasi-oscillatory manner with variability on the order of 3 to 7 years with further low-frequency modulation \citep{Sarachik2010}.
A \SI{100000}{year} integration of the Cane-Zebiak model \citep{Zebiak1987} was used to produce a monthly NINO3.4 index with stationary forcing as described in \citet{Ramesh2017}.
To create an annual time series, we average the October-December values of the NINO3 index for each year\footnote{So is this nino3.4 or 3?}.
A \SI{1000}{year} subset of this data is plotted in \cref{fig:enso-ts} along with the average spectral power of the series.

To generate $\mu(t)$, a sequence of length $M+N$ is randomly chosen from the \SI{100000}{year} NINO3 sequence.
Next, $\mu(t)$ is calculated deterministically from the NINO3 time series $x(t)$ by
\begin{equation}
  \mu(t) = \mu_0 + \beta x(t) + \gamma \qty(t - t_0)
\end{equation}
where $\beta$ controls the sensitivity to the NINO3 variation and $\gamma$ controls the magnitude of the secular trend ($\gamma=0$ gives a stationary series).

\subsubsection{Markov Chain\label{sec:methods-markov}}

To create sequences of floods with strong memory and regime-like behavior, we use a 2-state Markov chain to sample a sequence of states that are used to generate $\mu(t)$.
This Markov chain is given a constant probability of persistence which is the same for both states, such that the transition matrix is
\begin{equation*}
  P = \mqty[\pi & 1-\pi \\ 1-\pi & \pi]
\end{equation*}
This is first used to generate a sequence of states $S(t)$.
The value $\mu(t)$ depends only on the $S(t)$ and the time itself:
\begin{equation*}
  \mu(t) = \begin{cases}
    \mu_{0,1} + \gamma_1 \qty(t - t_0) & \qqtext{if} S(t) = 1 \\
    \mu_{0,2} + \gamma_2 \qty(t - t_0) & \qqtext{if} S(t) = 2
  \end{cases}
\end{equation*}
For simplicity, we assume that the coefficient of variation $\alpha$ is the same for both states.
We also assume $\mu_{0, 1} > \mu_{0, 2}$ so that state 1 can be interpreted as the ``wet'' state and state 2 as the ``dry'' state.

\subsection{Estimating Future Streamflow Sequences\label{sec:estimation}}

This paper does not propose new models or suggest a general ``best'' model; instead it aims to describe the total uncertainty in estimates using methods that have been well studied in the literature.

In general, each historical $N$-year sequence of streamflow is fit to a statistical model, from which 2000 sequences of future streamflow are generated using Monte Carlo simulation.
Three models are assessed for estimating future streamflow and are described in detail below.

\subsubsection{Stationary LN2 Model\label{sec:method-stationary}}

We first consider fitting a stationary model to the observed flood record, following classical assumptions of \gls{iid} sequences and stationarity.

We use the log-normal model (\cref{eq:lognormal}) for its simplicity, interpretability, and good performance relative to other 2-parameter models \citep{Vogel1996}.
For each simulation, the first $N$ years \(\qty(t=[t_0-N+1, \ldots, t_0])\) are treated as observations (denoted \(Q_{\text{hist}}\) for clarity).
Then, the parameters are fit in a Bayesian framework to fully represent the posterior uncertainty as implemented in the \texttt{stan} probabilistic programming language \citep{Carpenter2016}.
Weak (uninformative) priors are chosen; in the context of a synthetic data experiment using informative priors seems inappropriate but other studies\footnote{cite} have used scaling information and regional information to inform prior distributions and reduce posterior variance\footnote{Prob you can drop this aside -- might just serve as a distraction to the main point here}.
The full probability model is thus:
\begin{align}
  \log Q_\text{hist} & \sim \normal \qty(\mu, \sigma) \\
  \mu &\sim \normal \qty(0, 10) \\
  \sigma &\sim \normal \qty(0, 2)
\end{align}

\subsubsection{Trend LN2 Model\label{sec:method-trend}}

To incorporate secular change into a statistical model, many studies have used time parameterizations on certain parameters.
We consider a trend model where the data is log-normally distributed as \cref{eq:lognormal} with a linear time trend on both \( \mu \) and \( \sigma \).
As for the stationary case, we fit posterior samples in a Bayesian framework using \texttt{stan} and apply weak priors:
\begin{align}
  \log Q_\text{hist} & \sim \normal \qty(\mu_0 + \beta_\mu \qty(t - t_0), \sigma_0 + \beta_\sigma \qty(t-t_0)) \\
  \mu_0 & \sim \normal \qty(0, 10) \\
  \beta_\mu & \sim \normal \qty(0, 0.1) \\
  \sigma_0 & \sim \normal \qty(0, 2) \\
  \beta_\sigma & \sim \normal \qty(0, 0.05)
\end{align}
The influence of priors in this case is stronger than in the stationary case, and an alternative choice of priors might more strongly favor \(\beta_\mu=0\) and \(\beta_\sigma=0\), for example using the Horseshoe prior parameterization \citep{Piironen2016a}.

\subsubsection{Hidden Markov Model\label{sec:method-HMM}}

A Hidden Markov Model (HMM) is a statistical Markov model in which the system being modeled is assumed to be a Markov process with unobserved (\ie{} hidden) states \(S(t)\) \citep{Rabiner1986}.
The (unobserved) states evolve following a first-order Markov process (\(S(t)\) depends only on \(S(t-1)\)), and the observed variable (streamflow) depends only on the observed state.
As such, HMMs are a structured prediction method which extend general mixture models to sequences of data, where position in the sequence is relevant.
Because this structure allows the hidden state to represent hydroclimatological ``regimes'' \citep{Reinhold1982,Michelangeli1995,Merz2014}, HMMs have have been used extensively to model hydrological variables such as streamflow \citep[\ie{}][]{Bracken2016} or climate states such as ENSO.\footnote{Julian's work}

We fit observed streamflow sequences \(Q_\text{hist}\) using a HMM as implemented in the python \texttt{pomegranate} package \citep{Schreiber2016}.
The model is fit to the $N$ observed years of data using the Baum-Welch algorithm, assuming that the data follow a log-normal distribution conditional only on the state.
This algorithm simultaneously estimates the transition matrix of the Markov process and the conditional parameters of each distribution.
To choose the number of hidden states, a log probability score is used.\footnote{Need to implement this?}
Future floods are estimated by simulating future states from the resulting matrix and then drawing floods log-normally using the estimated parameters.

\subsection{Evaluating Fitting Models}

For a given choice of $M$, $N$, generating model, and fitting model, \(J\) sequences of streamflow of length \(M+N\) years each are generated.
For each of the \(J\) sequences, the fitting model is then fit on the $N$-year historical record.
\(K\) posterior simulations of future rainfall are then drawn from the fitted model.
We then perform a standard decomposition of uncertainty:
\begin{equation}
  \mathbb{E} \qty[ \qty( \hat{p}_T - p_T )^2 ] = \qty( \mathbb{E} \qty[ \hat{p}_T ] - p_T )^2 + \mathbb{E} \qty[ \qty( \hat{p}_T - \mathbb{E} \qty[ \hat{p}_T ] )^2]
\end{equation}
where the first term is the mean squared error, the second term is the bias squared, and the third term is the variance.

\subsection{Code Availability}

All computation was carried out in the \texttt{python} environment, making particular use of the \texttt{xarray}, \texttt{numpy}, \texttt{pandas}, and \texttt{matplotlib} libraries \citep{Hoyer2017,vanderWalt2011,McKinney2010,Hunter2007}.
The codes used to generate the figures and text of this paper are available at \url{github.com/jdossgollin/MNpaper}.\footnote{It's currently a private repository}

% -----------------------------------------------------------------------------
% ESTIMATION BIAS AND UNCERTAINTY
% -----------------------------------------------------------------------------

\section{Estimation Bias and Uncertainty}\label{sec:results}

We begin by considering how different statistical models for estimating future flood sequences perform under stationary conditions and when a secular trend is introduced.

\subsection{Stationary Case}\label{sec:results-stationary}

We first consider the estimation bias and variance as a function of $M$ and $N$ when the true generative process is stationary.

To do this we generate 1000 sequences of streamflow for each combination of $M$, $N$, and generative process.
The parameters for the NINO3 and Markov models are given in \cref{tab:nino-stationary}.
We use a threshold $Q_T = \SI{5000}{units}$.
We then fit each estimation model to each sequence and draw 2000 estimates of future streamflow from the fitted model.
These are then used to determine $\hat{p}_T$, allowing calculation of the expected bias and variance in its estimation for each combination of $M$, $N$, generative model, and estimating model.

\begin{table}[ht]
  \begin{tabular}{L{1in}L{3.75in}L{0.75in}}
    \toprule
    Paramater & Description & Value \\
    \midrule
    $\mu_0$ & Expected value of $\log Q$ when $x=0$ & 6 \\
    $\beta_\mu$ & Coefficient of $x$ on $\mu$ & 0.5 \\
    $\alpha$ & Coefficient variation & 0.1 \\
    $\sigma_\text{min}$ & Lowest allowable value of $\sigma$ & 0.01 \\
    $\gamma$ & Time trend of $\mu$ & 0 \\
    \bottomrule
  \end{tabular}
  \caption{Parameters of NINO3 model under stationary case. $x(t)$ denotes the modeled NINO3 time series.}\label{tab:nino-stationary}
\end{table}
\begin{table}[ht]
  \begin{tabular}{L{1in}L{3.75in}L{0.75in}}
    \toprule
    Parameter & Description & Value \\
    \midrule
    $\mu_1$ & Expected value of $\log Q$ for state 1 & 6.75 \\
    $\mu_2$ & Expected value of $\log Q$ for state 2 & 6 \\
    $\gamma_1$ & Time trend of $\mu_1$ & 0 \\
    $\gamma_2$ & Time trend of $\mu_2$ & 0 \\
    $\alpha$ & Coefficient variation & 0.1 \\
    $\sigma_\text{min}$ & Lowest allowable value of $\sigma$ & 0.01 \\
    \bottomrule
  \end{tabular}
  \caption{Parameters of Markov model under stationary case}\label{tab:markov-stationary}
\end{table}

\Cref{fig:stationary-bias}\footnote{It's more intuitive for me if the M axis is increasing upwards, but perhaps there is a reason for your orientation that I am not considering} shows that in general, bias is quite similar across generative models but varies substantially between the models used for estimation.
The HMM and LN2 Stationary models have no time term, and so their bias is independent of $M$.
In general both of these models have very low bias, though the LN2 Stationary model tends to over-estimate the probability of flooding for very short $N$, unsurprising since the true distribution of streamflow is bimodal for both generating functions. 
The LN2 trend model registers a non-trivial probability of a positive time trend, particularly for short $N$, leading to large bias in estimation.
For very long $M$ the bias is higher, as estimated probability of exceeding the threshold is exceeded.
\begin{figure}[ht]
  \includegraphics[width=\textwidth]{bias_stationary.pdf}
  \caption{The estimation bias for 1000 sequences generated by each of two stationary generating functions (row) and fit by each of three fitting functions (columns) as a function of $M$ and $N$.\label{fig:stationary-bias}}
\end{figure}

The variance estimates show similar patterns to those of the bias and are shown in \cref{fig:stationary-variance} as the log standard deviation.
As suggested in the bias plots, the HMM and LN2 Stationary models have very low variance of their estimates.
The LN2 Trend model has much higher variance in estimates, particularly for short $N$ and long $M$.
\begin{figure}[ht]
  \includegraphics[width=\textwidth]{variance_stationary.pdf}
  \caption{The estimation variance for 1000 sequences generated by each of two stationary generating functions (row) and fit by each of three fitting functions (columns) as a function of $M$ and $N$.\label{fig:stationary-variance}}
\end{figure}

\subsection{Trend Case}

We next repeat the previous example but introduce a time trend in the data.
Thus the parameters of the NINO3 model are as \cref{tab:nino-stationary} except that $\gamma=0.015$ and the parameters of the Markov model are as \cref{tab:markov-stationary} except that $\gamma_1 = 0.015$.

For short $M$, particularly when $N$ is not too big, the stationary models (HMM and LN2 Stationary) perform well for both generative functions.
As $M$ increases, they begin to substantially underestimate the probability of future flooding.
As for the stationary case, when $N$ is short the LN2 Stationary model tends to over-estimate $\hat{p}_T$ because it sees variation caused by the NINO3 time series or the Markov state switching and assigns it to internal variability, leading to unrealistically fat tails in the estimate.
The trend model, by contrast, over-estimates $p_T$ except when $N$ is very large, mainly because the posterior uncertainty in the magnitude of the trend means that the model believes that a trend larger than observed is not unlikely.
\begin{figure}[ht]
  \includegraphics[width=\textwidth]{bias_trend.pdf}
  \caption{The estimation bias for 1000 sequences generated by each of two non-stationary generating functions (row) and fit by each of three fitting functions (columns) as a function of $M$ and $N$.\label{fig:trend-bias}}
\end{figure}

\begin{figure}[ht]
  \includegraphics[width=\textwidth]{variance_trend.pdf}
  \caption{The estimation variance for 1000 sequences generated by each of two non-stationary generating functions (row) and fit by each of three fitting functions (columns) as a function of $M$ and $N$.\label{fig:trend-variance}}
\end{figure}

\begin{figure}
  \includegraphics[width=\textwidth]{example_long.pdf}
  \caption{
    A single sequence of streamflow generated by the NINO3 function with (L) $\gamma=0$ and (R) $\gamma=0.015$. 
    The sequence is fit to each fit to three different fitting models (rows); the grey shading indicates the posterior 50\%, 80\%, and 99\% confidence intervals of $\hat{P}_T(t)$.
    Here $M=N=150$.\label{fig:example-long}
  }
\end{figure}

\begin{figure}
  \includegraphics[width=\textwidth]{example_short.pdf}
  \caption{
    As \cref{fig:example-short} except \(N=50\).\label{fig:example-short}
  }
\end{figure}

% -----------------------------------------------------------------------------
% CASE STUDY
% -----------------------------------------------------------------------------

\section{Case Study}

I would go back to a case study and look at a 500-year streamflow sample (could be generated synthetically if we can't get a good real one).
Fix $N=100$ (say) and choose between $M=5$ or $M=50$. 
Estimate this model with 
\begin{enumerate}[label=(\roman*)]
  \item HMM;
  \item LN2 Stationary;
  \item LN2 Trend; and
  \item Perfect Information (\ie{} knowing the future exactly)
\end{enumerate}
Then at each time, calculate the insurance price for each model from \cref{eq:insurance-price} (the perfect information model will have a risk premium of zero) and summarize the over or under-pricing of the risk premium.
Then explain that one would be unlikely to buy a 50-year insurance contract but that it's helpful to compare the two models using equal criteria.
We could extend to include NPV analysis if desired.

\emph{Please comment: should this be part of our paper?}\footnote{I vote yes to include this, since I think it nails the "so-what?" part}

% -----------------------------------------------------------------------------
% DISCUSSION
% -----------------------------------------------------------------------------

\section{Discussion}\footnote{Not doing much editing from here on yet, since I think it will depend a bit on a more finalized version of the above sections. The beginning of the discussion does read as introductory material to me -- likely a case study in between sections 3 and 5 will help ease the reader from the discussion of bias and variance for N and M (without much context) in section 3 into the very applied considerations discussed at the beginning of section 5. Still I do think that the discussion could use some re-focusing. Also, adding some context in the results section will help to set up a productive discussion I think.}


Infrastructure in the developed world is aging \citep{Ho2017}, leading to questions of whether and how to replace it.
In the developed world new infrastructure is being built.\footnote{citations}
Given a positive future discount rate and uncertainty associated with future urbanization, economic growth, and climate change, the expected net benefits of this infrastructure are highly uncertain.
At the same time the private sector has accepted an increasing share of global risk \citep{WorldEconomicForum2016}, which it has mitigated primarily through financial instruments.
How should different contracts be priced? When does it make sense to build large infrastructure?
These questions cannot be answered with existing methodologies.

The assessment of sampling bias and variance associated with estimating the occurrence probability of an extreme event, $\hat{p}_T$, maps directly into this insurance contract, where a typical price for a $M$-year contract is given by
\begin{equation}\label{eq:insurance-price}
  \frac{\mathbb{C}}{M} = \lambda \qty(\mathbb{E} \qty[p_T] + c \mathbb{V} \qty[p_T])
\end{equation}
where $\mathbb{E}$ denotes the expectation operator, $\mathbb{V}$ the variance operator, $\lambda$ the profit margin, and $c$ the risk premium.
Bias in $\hat{P}_T$ leads to inefficient pricing, since if the price is too low, the insurer is exposed to failure and if it is too high the buyer will be less inclined to purchase insurance.

We illustrated sampling uncertainty and inter-model disagreement using several simple models for annual-maximum floods.
In more realistic applications, analysis of flood frequency can be improved by combining information across space \citep{Lima2016,Merz2008a}, combining model forecasts, \etc{}.
Analysis of other weather and climate hazards may use different distributions, and the time structure of the underlying climate process may be different.

For the type of extreme flood events we are interested in, the nominal return period at some location flooded during the event will typically be larger than 10 years.
While this is a relatively rare event at a given location, across a geography as large as the United States, one can expect to see such events rather frequently, emphasizing the need to understand and predict the risk of such events over a season or few years to facilitate preparation and mitigation strategies.
Recent efforts\footnote{Prad's work} to identify regions where risk of a particular event is negatively correlated in space.

Such clustering in the occurrence of extreme rainfall and drought is manifest in the potential exposure of mining companies, who face spatial as well as temporal clustering \citep{Bonnafous2017a,Bonnafous2017b}.

How could  financial risk mitigation instruments be used in conjunction with structural design to identify a robust, adaptive climate risk mitigation strategy with sequential decisions that are informed by updated estimates of risk.

Since we know the true distribution of the flood sequences (we have generated them ourselves), using the log-normal for both generating and fitting enables interpretation of results as a lower bound on total uncertainty.

From a decision making perspective, one could either design and build a project that gives us protection through the year 2100 considering both types of nonstationarities, or consider a sequence of projects every M years that incrementally add (or not) protection. How does the uncertainty and hence risk for these decisions manifest depending on the design level p, the duration M, and the uncertainty associated with the estimation of the \( f(Qp(t)) \) given a methodology used for prediction -- \eg{} the model chain using gcms and such and models fitted to a historical data length $M$.
What is the implication for choices of p and M under different models for stationarity and uncertainty using some simple loss and cost  functions.
If a positive discount rate is used for the future, as is typical in public and private sector planning models,\footnote{CITE} the investment returns on major public and private investment may diminish substantially far into the future.
This consideration suggests that it may be more effective to understand the consequences for analysis focusing on shorter lived mitigation alternatives that may last for 1, 5, 10, 20, 30, or 50 years.
Thus, sequences of such smaller projects may in some cases better reflect climate adaptation goals, particularly given the possibilities that major climate change impacts in a particular place may occur far into the future; that climate mitigation efforts may be successful; or that the needs of human society may change drastically over these horizons.


% -----------------------------------------------------------------------------
% SUMMARY
% -----------------------------------------------------------------------------

\section{Summary}

The main point you are hitting is that the very nature of climate has long memory and some people point this out, many model it, and most future projections don't show this.
All corrections (even if we like them) address the marginal distribution.
So while the debate you refer to is interesting to a degree, the point we are making is that in terms of risk analysis it is not even tangentially relevant to reality and what we bring up -- our focus -- is strictly the risk based decision process and we are agnostic to the details of the models, other than the long memory features and spatial teleconnections associated with them that are seen as marks of the system behavior.
We actually demonstrate that the potential loss in a 2100 scenario has to be catastrophic and really out of the range of any historical variability for it to be relevant.
If we accept this proposition then we have a strong case for the mitigation of climate change but a much weaker case for adaptation, since if we are not able to mitigate that scenario then building a giant wall to control the 2100 scenario today will be economically and socially infeasible anyway.
So the next obvious question is in which situation would waiting and updating the decision make sense and this can be set up and explored or just discussed.

To really drive this paper home, we ought to end with a few recommendations for practitioners estimating and adapting to future climate risk such as:
\begin{itemize}
  \item For some combinations of $M$ and $N$, use of trend models leads to sub-optimal decisions \emph{even when there is a truely linear trend and the model is correctly specified}
  \item Consider the structure of the underlying trend, $N$, and $M$ when choosing a model
  \item Consider the bias-variance tradeoff of model complexity
\end{itemize}
We could even offer some recommendations as to when to choose ``insurance'' projects versus ``infrastructure'' projects, but this may just needlessly antagonize readers.

% -----------------------------------------------------------------------------
% END HERE
% -----------------------------------------------------------------------------

\appendix

\section{Thanks}

The authors thank Nandini Ramesh of Columbia University for providing the synthetic NINO3 index from a \SI{1000000}{year} run of the Cane-Zebiak model as described in \citet{Ramesh2017}.
The authors thank Columbia ITS for support running simulations on the cluster.
James thanks NSF.
Dave thanks grant.

\subsection{Are Generated Sequences Realistic?\label{sec:sequence-realistic}}

The models used to generate synthetic flood sequences used simple dynamics, but nevertheless were designed to incorporate regime-like and quasi-period variability at multiple timescales.

To verify that these synthetic streamflow sequences sufficiently represent these behaviors, we plot several sequences.
The ENSO model used a long run of the Cane-Zebiak model \citep{Zebiak1987,Ramesh2017} to determine \(\mu(t)\); a \SI{2500}{year} sub-set of the annualized time series is shown in \cref{fig:enso-ts}.
As described in the literature,\footnote{citations, and maybe some paleo stuff as well} this model shows quasi-periodic oscillations in the \SIrange{3}{7}{year} band but with substantial low-frequency modulation of extreme activity.
The index is also asymmetrically distributed, with a longer tail on the positive (El Ni\~{n}o) side than the negative (La Ni\~{n}a) phase, which is consistent with observations of ENSO.
Analysis of the power density spectrum shows a strong peak around 3-6 years.
\begin{figure}
  \includegraphics[width=\textwidth]{enso.pdf}
  \caption{
    (L): A \SI{1000}{year} sub-set of the synthetic time series described in \cref{sec:methods-nino}.
    (R): The power spectral density of the annualized time series. A moving window was applied to the density for smoothing.\label{fig:enso-ts}
  }
\end{figure}

\Cref{fig:stationary-sequences} shows 50 streamflow sequences generated with $M=50$ and $N=100$.
The ENSO sequences were generated with the parameters shown in \cref{tab:nino-stationary} Markov sequences were generated with the parameters shown in \cref{tab:markov-stationary}.
\begin{figure}[b]
  \includegraphics[width=\textwidth]{sequences_stationary.pdf}  
  \caption{
    50 streamflow sequences from (Top) the ENSO-based model described in \cref{sec:methods-nino} and (Bottom) the Markov chain modle described in \cref{sec:methods-markov}.
    Both models use a stationary parameterization $\gamma=0$.
    49 of the 50 sequences are plotted in gray, and one (randomly chosen) is plotted in blue for emphasis.
    Note that $y$-axis has a log scale.\label{fig:stationary-sequences}
  }
\end{figure}

To incorporate secular variability into our model, we repeat the exercise but add a trend term to the data.
For the ENSO time series, this mean setting \(\gamma=0.015\) while for the Markov case this meant a trend term on the wet state only, so that \(\gamma_1=0.015\).
The resulting sequences are shown in \cref{fig:trend-sequences}.
The trend has an extremely large impact on the probability of an extremely high flood, but the underlying time series structure is generally preserved.
\begin{figure}[b]d
  \includegraphics[width=\textwidth]{sequences_trend.pdf}
  \caption{
    As \cref{fig:stationary-sequences} but a trend term has been added by setting \(\gamma=0.015\).\label{fig:trend-sequences}
  }
\end{figure}

This choice of parameterizing secular change as a linear trend on the expected value of log streamflow was made to aid interpretability, and not because of any particular physical reasoning.
Indeed, for large $N$ this model suggests implausibly dry conditions and implausibly wet conditions for very large $M$.
However, given widespread use of trend-based models in the flood frequency analysis literature it is informative to consider the ``best-case'' scenario in which the model is correctly specified and there is a true, linear trend in the data.

\section{Notation}

\begin{description}
  \item[\( Q(t) \)] Annual-maximum flood series
  \item[\( T \)] Return period, in years
  \item[\( Q_T \)] Flood threshold
  \item[\( M \)] the project planning period
  \item[\( N \)] the length of the observational record, in years
  \item[\( x(t) \)] the annualized time series of the NINO3 index
  \item[\( p_T \)] the true probability that \( Q(t) \geq Q_T \) in the future $M$-year period
  \item[\( \hat{p}_T \)] the estimated probability that \( Q(t) \geq Q_T \) in the future $M$-year period
\end{description}

\printbibliography{}


\end{document}