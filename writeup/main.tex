\documentclass[12pt]{article}

\input{header/macros.tex}
\input{header/packages.tex}

\title{Planning for Quasi-Periodic and Secular Variability of Climate Risk}
\author[1,2]{James Doss-Gollin}
\author[1,2]{David Farnham}
\author[1,2]{Upmanu Lall}
\affil[1]{Columbia Water Center, Columbia University}
\affil[2]{Department of Earth and Environmental Engineering, Columbia University}

\input{header/glossaries.tex}
\input{header/biblatex.tex}
\addbibresource{library.bib}
\graphicspath{{../figs/}}

% -----------------------------------------------------------------------------
% AUTHOR AND TITLE AND TITLEPAGE
% -----------------------------------------------------------------------------

\begin{document}
\maketitle
\RaggedRight{}

\begin{abstract}
  Recent disasters highlight the need for improved adaptation to extreme floods and other climate hazards.
  Climate risk mitigation has traditionally been achieved through large-scale infrastructure projects with long project lifespans.
  However, political and economic constraints limit the ability of central governments to fund such projects, leading to an increased role for the private sector and local governments \citep{WorldEconomicForum2016}.
  These actors typically operate on much shorter planning horizons for which the quasi-periodic variations of hydroclimate systems may be of interest.
  We use a series of structured experiments to explore how estimated and true climate risk vary as a function of the project planning period and the available data record.
  Strong differences in the expected value of flood risk, and the associated uncertainty, emerge between long planning periods dominated by secular trend and short planning periods dominated by quasi-periodic variability.
  Successful climate adaptation strategies should consider decisions on multiple timescales.
\end{abstract}

% -----------------------------------------------------------------------------
% INTRODUCTION
% -----------------------------------------------------------------------------

\section{Introduction}\label{sec:introduction}

The \nth{20} century saw the evolution of a nearly universal approach to the statistical analysis of weather and climate hazard.
In this framework, event risk is assessed as a relationship between the event magnitude and its annual probability of exceedance.
Under the assumption that the historical or simulated record of events constitutes an stationary, \gls{iid} process, and using an appropriate statistical inference model, an estimate of this relationship and its uncertainty based on a finite $N$-year record is developed.
This is then used as a measure for decision analysis for infrastructure, flood zoning and insurance instruments for event risk mitigation applied to the costs and residual losses expected from the mitigation instrument.
The uncertainty associated with the model may be used to develop a risk premium for insurance, or for assessing the potential over- or under- design of infrastructure.

Over time it has become clear that even small fluctuations in the dynamics of the hydroclimate system, known as nonstationarities \citep{Milly2008}, can lead to clustering of extreme events  and strong time dependence of a public or private actor's risk profile \citep{Bonnafous2017a,Bonnafous2017b}.
For example, in 2011 four typhoons struck northern Thailand \citep{Gale2013}, causing severe damage to public and private infrastructure and crippling global supply chains \citep{Haraguchi2015}.
Similarly, in June 2013 three summertime cyclones followed highly similar tracks, leading to widespread flooding in central Europe \citep{Grams2014}.
More recently the 2017 Atlantic hurricane season led to over \usd{130 billion} in insured losses \citep{SwissRe2017}, despite negligible losses in the preceding several years.
Each of these events was highly improbable under \gls{iid} assumptions, yet the clustering of such events is well-documented in the literature \citep{Hurst1951,Matalas2012,Merz2014} and is consistent with our understanding of the nonlinear dynamics of the climate system \citep{Hannachi2017}.

Nonstationarity behavior that leads to violation of traditional assumptions takes two primary forms.
The first, secular change, may be related to anthropogenic climate change, river modification, or land use change \citep{Merz2014}.
The uncertainties associated with this are not purely informational, at least in the sense that they can be inferred from a historical sample or model.
Both nature and human actions over the course of this century will determine the magnitude and direction of secular change, and neither is fully predictable.
The uncertainty associated with secular change clearly grows as one looks out into the distant future, and the importance of investment decisions that may engage us in considerations of designs that go far out into the future, \eg{} 2100, diminish if any positive discount rate is used for the future.
While this consideration offsets the impact of the growing uncertainty for longer project life, it also suggests that it may be more effective to understand the consequences for analysis that may focus on shorter lived adaptation strategies which may last for 1, 5, 10, 20,  30 or 50 years.

Focusing on these shorter time horizons brings up the concern associated with the second type of nonstationarity in climate: the quasi-periodic inter-annual, decadal and multidecadal modes of climate variability that influence the spatiotemporal expression of climate extremes in ways that violate the standard \gls{iid} assumption \citep{Matalas2012,Hannachi2017}.
For example, analysis of a long synthetic data set demonstrated that using the past $N$ years to estimate return periods for the next $N$ years leads to a high degree of surprise even when secular change is exactly zero \citep{Jain2001}.
The finding that floods at a particular location are clustered in time relative to what could be expected under an \gls{iid} process was replicated for an observed long streamflow record \citep{Jain2001} and is consistent with analysis of a \SI{1500}{year} streamflow reconstructions \citep{Swierczynski2012}.
Analysis of annual-maximum rainfall events at sites around the world has also demonstrated a high degree of spatial clustering \citep{Bonnafous2017a}.

Broad recognition of nonstationarity has led to a plethora of efforts aimed at identifying the nature of the nonstationary and modeling it, though no single approach has proven broadly successful.
Such efforts have included time trends on statistical parameters \citep{Strupczewski2001,Vogel2011,Obeysekera2014,Serinaldi2015}, climate covariates \citep{Griffis2007,Sun2014,Hall2014,Delgado2014,Silva2016}, latent variable or state-based modeling approaches \citep{Waylen1986,Sveinsson2005,Griffis2007}, or using only the last $N$ years of the data.\footnote{Rich Vogel citation?}
Each model will necessarily give different estimates, which may fit the historical record with comparable accuracy but diverge at an increasing rate as the model is forecast further into the future.
Despite the wide suite of available models, the question of how one should use these nonstationarity estimates for the design of risk mitigation infrastructure or the pricing of insurance instruments has been much less studied.

A critical question in this regard is how to assess whether any structure designed using these projections may actually perform in the future relative to the proposed level of risk protection.
These questions go well beyond what can be reliably answered at this time.
Clearly, in a nonstationary world, the probability of exceedance could be considered to be temporally variable over the project life.
This would however, need to be a prediction of that probability ex ante, using appropriate modeling tools applied to the historical data or to climate model based forecasts.
The risk estimation from the historical record must explicitly inform a risk prediction over the future planning period, and depending on the approach and the form of the underlying nonstationarity, we can expect both bias and uncertainty associated with the prediction, and we need temporally varying probabilities to address the inevitable risk clustering.
This is then the statement of the risk mitigation design problem that we need to consider for nonstationary climate.

In this paper we provide some initial insight into the bias and uncertainty associated with flood frequency estimation for projects as a function of the project planning period $M$ as well as the length of the historical record $N$.
The project planning period can be interpreted as the design life of a physical structure, or as the contract length of a financial instrument.
We focus on structured experiments with synthetic data on extremes to illustrate how one could approach the analyses and what the implications are of varying $N$ and $M$ on the ability to do effective inference and on the ability to judge operational performance.
Though the specific examples target flood risk, this method and framework may be used for understanding the implications of uncertain projections of the future for climate hazards including drought, hurricanes, and wind. \footnote{Overall comment on the introduction:  We may consider hinting at some of the results a bit more, otherwise the introduction is fairly wide open and I'm not sure the reader gets a good sense for what she is in for... but then again, perhaps that's OK}.

% -----------------------------------------------------------------------------
% METHODS
% -----------------------------------------------------------------------------

\section{Methods}\label{sec:methods}

We use a statistical framework with highly idealized models to generate sequences of annual-maximum floods, fit these sequences to probabilistic models that incorporate time in different ways, and evaluate the performance of these models for multiple observational record lengths $N$ and project planning periods $M$.

\subsection{Generating Synthetic Flood Sequences\label{sec:methods-generating}}

We generate synthetic flood sequences from two different simple conceptual models that represent chaotic dynamics and long memory.
For both cases, the expected value of the annual maximum flood follows a log-normal distribution, conditional on an expected value $\mu(t)$, which in turn depends on individual models described in \cref{sec:methods-nino,sec:methods-markov}:
\begin{equation} \label{eq:lognormal}
  \log Q(t) \big| \mu(t),\sigma(t) \sim \normal \qty(\mu(t), \sigma(t)).
\end{equation}
For simplicity we assume a constant coefficient of variation:
\begin{equation}\label{eq:calc-sigma}
  \sigma(t) = 
  \begin{cases} 
    \alpha \mu(t) & \qqtext{if} \alpha \mu(t) \geq \sigma_\text{min} \\ 
    \sigma_\text{min} & \qqtext{else}
  \end{cases}
\end{equation}
where $\sigma_\text{min}$ is a small constant that ensures non-trivial noise in the system.

The procedure to generate a sequence of streamflow is thus:
\begin{enumerate}
  \item Choose $t_0$, $M$, and $N$ so that the time is $\vb{t}=t_0-N+1, \ldots, t_0+M$.
  \item Use one of the models described below to simulate $\mu(t)$
  \item Use \cref{eq:calc-sigma} to calculate $\sigma(t)$
  \item For each time $t \in \vb{t}$, draw $Q(t)$ following \cref{eq:lognormal}.
\end{enumerate}
Though both approaches used to generate sequences $\mu(t)$ use highly simplified models neglecting many important mechanisms, they enable us to generate many synthetic sequences of $\qty[M + N]$ years which exhibit chaotic but organized quasi-periodic behavior (regime behavior or low-frequency variability) plus a trend term, thereby capturing some essential features of hydrological variability.

\subsubsection{Synthetic NINO3 Model\label{sec:methods-nino}}

ENSO is the dominant mode of tropical climate variability, impacts flood risk around the world, and varies in a chaotic, quasi-oscillatory manner with variability on the order of 3 to 7 years with further low-frequency modulation \citep{Sarachik2010}.
A \SI{100000}{year} integration of the Cane-Zebiak model \citep{Zebiak1987} was used to produce a monthly NINO3.4 index with stationary forcing as described in \citet{Ramesh2017}.
To create an annual time series, we average the October-December values of the NINO3 index for each year\footnote{So is this nino3.4 or 3?}.
A \SI{1000}{year} subset of this data is plotted in \cref{fig:enso-ts} along with the average spectral power of the series.

To generate $\mu(t)$, a sequence of length $M+N$ is randomly chosen from the \SI{100000}{year} NINO3 sequence.
Next, $\mu(t)$ is calculated deterministically from the NINO3 time series $x(t)$ by
\begin{equation}\label{eq:nino3}
  \mu(t) = \mu_0 + \beta x(t) + \gamma \qty(t - t_0)
\end{equation}
where $\beta$ controls the sensitivity to the NINO3 variation and $\gamma$ controls the magnitude of the secular trend ($\gamma=0$ gives a stationary series).

\subsubsection{Markov Chain\label{sec:methods-markov}}

To create sequences of floods with strong memory and regime-like behavior, we use a 2-state Markov chain to sample a sequence of states that are used to generate $\mu(t)$.
This Markov chain is given a constant probability of persistence which is the same for both states, such that the transition matrix is
\begin{equation*}
  P = \mqty[\pi & 1-\pi \\ 1-\pi & \pi]
\end{equation*}
This is first used to generate a sequence of states $S(t)$.
The value $\mu(t)$ depends only on the $S(t)$ and the time itself:
\begin{equation*}
  \mu(t) = \begin{cases}
    \mu_{0,1} + \gamma_1 \qty(t - t_0) & \qqtext{if} S(t) = 1 \\
    \mu_{0,2} + \gamma_2 \qty(t - t_0) & \qqtext{if} S(t) = 2
  \end{cases}
\end{equation*}
For simplicity, we assume that the coefficient of variation $\alpha$ is the same for both states.
We also assume $\mu_{0, 1} > \mu_{0, 2}$ so that state 1 can be interpreted as the ``wet'' state and state 2 as the ``dry'' state.

\subsection{Estimating Future Streamflow Sequences\label{sec:estimation}}

This paper does not propose new models or suggest a general ``best'' model; instead it aims to describe the total uncertainty in estimates using methods that have been well studied in the literature.

In general, each historical $N$-year sequence of streamflow is fit to a statistical model, from which 2000 sequences of future streamflow are generated using Monte Carlo simulation.
Three models are assessed for estimating future streamflow and are described in detail below.

\subsubsection{Stationary LN2 Model\label{sec:method-stationary}}

We first consider fitting a stationary model to the observed flood record, following classical assumptions of \gls{iid} sequences and stationarity.

We use the log-normal model (\cref{eq:lognormal}) for its simplicity, interpretability, and good performance relative to other 2-parameter models \citep{Vogel1996}.
For each simulation, the first $N$ years \(\qty(t=[t_0-N+1, \ldots, t_0])\) are treated as observations (denoted \(Q_{\text{hist}}\) for clarity).
Then, the parameters are fit in a Bayesian framework to fully represent the posterior uncertainty as implemented in the \texttt{stan} probabilistic programming language \citep{Carpenter2016}.
Weak (uninformative) priors are chosen; in the context of a synthetic data experiment using informative priors seems inappropriate but other studies\footnote{cite} have used scaling information and regional information to inform prior distributions and reduce posterior variance\footnote{Prob you can drop this aside -- might just serve as a distraction to the main point here}.
The full probability model is thus:
\begin{align}
  \log Q_\text{hist} & \sim \normal \qty(\mu, \sigma) \\
  \mu &\sim \normal \qty(0, 10) \\
  \sigma &\sim \normal \qty(0, 2)
\end{align}

\subsubsection{Trend LN2 Model\label{sec:method-trend}}

To incorporate secular change into a statistical model, many studies have used time parameterizations on certain parameters.
We consider a trend model where the data is log-normally distributed as \cref{eq:lognormal} with a linear time trend on both \( \mu \) and \( \sigma \).
As for the stationary case, we fit posterior samples in a Bayesian framework using \texttt{stan} and apply weak priors:
\begin{align}
  \log Q_\text{hist} & \sim \normal \qty(\mu_0 + \beta_\mu \qty(t - t_0), \sigma_0 + \beta_\sigma \qty(t-t_0)) \\
  \mu_0 & \sim \normal \qty(0, 10) \\
  \beta_\mu & \sim \normal \qty(0, 0.1) \\
  \sigma_0 & \sim \normal \qty(0, 2) \\
  \beta_\sigma & \sim \normal \qty(0, 0.05)
\end{align}
The influence of priors in this case is stronger than in the stationary case, and an alternative choice of priors might more strongly favor \(\beta_\mu=0\) and \(\beta_\sigma=0\), for example using the Horseshoe prior parameterization \citep{Piironen2016a}.

\subsubsection{Hidden Markov Model\label{sec:method-HMM}}

A Hidden Markov Model (HMM) is a statistical Markov model in which the system being modeled is assumed to be a Markov process with unobserved (\ie{} hidden) states \(S(t)\) \citep{Rabiner1986}.
The (unobserved) states evolve following a first-order Markov process (\(S(t)\) depends only on \(S(t-1)\)), and the observed variable (streamflow) depends only on the observed state.
As such, HMMs are a structured prediction method which extend general mixture models to sequences of data, where position in the sequence is relevant.
Because this structure allows the hidden state to represent hydroclimatological ``regimes'' \citep{Reinhold1982,Michelangeli1995,Merz2014}, HMMs have have been used extensively to model hydrological variables such as streamflow \citep[\ie{}][]{Bracken2016} or climate states such as ENSO.\footnote{Julian's work}

We fit observed streamflow sequences \(Q_\text{hist}\) using a HMM as implemented in the python \texttt{pomegranate} package \citep{Schreiber2016}.
The model is fit to the $N$ observed years of data using the Baum-Welch algorithm, assuming that the data follow a log-normal distribution conditional only on the state.
This algorithm simultaneously estimates the transition matrix of the Markov process and the conditional parameters of each distribution.
To choose the number of hidden states, a log probability score is used.\footnote{Need to implement this?}
Future floods are estimated by simulating future states from the resulting matrix and then drawing floods log-normally using the estimated parameters.

\subsection{Evaluating Fitting Models}

For a given choice of $M$, $N$, generating model, and fitting model, \(J\) sequences of streamflow of length \(M+N\) years each are generated.
For each of the \(J\) sequences, the fitting model is then fit on the $N$-year historical record.
\(K\) posterior simulations of future rainfall are then drawn from the fitted model.
We then perform a standard decomposition of uncertainty:
\begin{equation}
  \mathbb{E} \qty[ \qty( \hat{p}_T - p_T )^2 ] = \qty( \mathbb{E} \qty[ \hat{p}_T ] - p_T )^2 + \mathbb{E} \qty[ \qty( \hat{p}_T - \mathbb{E} \qty[ \hat{p}_T ] )^2]
\end{equation}
where the first term is the mean squared error, the second term is the bias squared, and the third term is the variance.

\subsection{Code Availability}

All computation was carried out in the \texttt{python} environment, making particular use of the \texttt{xarray}, \texttt{numpy}, \texttt{pandas}, and \texttt{matplotlib} libraries \citep{Hoyer2017,vanderWalt2011,McKinney2010,Hunter2007}.
The codes used to generate the figures and text of this paper are available at \url{github.com/jdossgollin/MNpaper}.\footnote{It's currently a private repository}

% -----------------------------------------------------------------------------
% ESTIMATION BIAS AND UNCERTAINTY
% -----------------------------------------------------------------------------

\section{Estimation Bias and Uncertainty}\label{sec:results}

We begin by considering how different statistical models for estimating future flood sequences perform under stationary conditions and when a secular trend is introduced.

\subsection{Stationary Case}\label{sec:results-stationary}

We first consider the estimation bias and variance as a function of $M$ and $N$ when the true generative process is stationary.

To do this we generate 1000 sequences of streamflow for each combination of $M$, $N$, and generative process.
The parameters for the NINO3 and Markov models are given in \cref{tab:nino-stationary}.
We use a threshold $Q_T = \SI{5000}{units}$.
We then fit each estimation model to each sequence and draw 2000 estimates of future streamflow from the fitted model.
These are then used to determine $\hat{p}_T$, allowing calculation of the expected bias and variance in its estimation for each combination of $M$, $N$, generative model, and estimating model.

\begin{table}[ht]
  \begin{tabular}{L{1in}L{3.75in}L{0.75in}}
    \toprule
    Paramater & Description & Value \\
    \midrule
    $\mu_0$ & Expected value of $\log Q$ when $x=0$ & 6 \\
    $\beta_\mu$ & Coefficient of $x$ on $\mu$ & 0.5 \\
    $\alpha$ & Coefficient variation & 0.1 \\
    $\sigma_\text{min}$ & Lowest allowable value of $\sigma$ & 0.01 \\
    $\gamma$ & Time trend of $\mu$ & 0 \\
    \bottomrule
  \end{tabular}
  \caption{Parameters of NINO3 model under stationary case. $x(t)$ denotes the modeled NINO3 time series.}\label{tab:nino-stationary}
\end{table}
\begin{table}[ht]
  \begin{tabular}{L{1in}L{3.75in}L{0.75in}}
    \toprule
    Parameter & Description & Value \\
    \midrule
    $\mu_1$ & Expected value of $\log Q$ for state 1 & 6.75 \\
    $\mu_2$ & Expected value of $\log Q$ for state 2 & 6 \\
    $\gamma_1$ & Time trend of $\mu_1$ & 0 \\
    $\gamma_2$ & Time trend of $\mu_2$ & 0 \\
    $\alpha$ & Coefficient variation & 0.1 \\
    $\sigma_\text{min}$ & Lowest allowable value of $\sigma$ & 0.01 \\
    \bottomrule
  \end{tabular}
  \caption{Parameters of Markov model under stationary case}\label{tab:markov-stationary}
\end{table}

\Cref{fig:stationary-bias} shows that in general, bias is quite similar across generative models but varies substantially between the models used for estimation.
The HMM and LN2 Stationary models have no time term, and so their bias is independent of $M$.
In general both of these models have very low bias, though the LN2 Stationary model tends to over-estimate the probability of flooding for very short $N$, unsurprising since the true distribution of streamflow is bimodal for both generating functions. 
The LN2 trend model registers a non-trivial probability of a positive time trend, particularly for short $N$, leading to large bias in estimation.
For very long $M$ the bias is higher, as estimated probability of exceeding the threshold is exceeded.
\begin{figure}[ht]
  \includegraphics[width=\textwidth]{bias_stationary.pdf}
  \caption{The estimation bias for 1000 sequences generated by each of two stationary generating functions (row) and fit by each of three fitting functions (columns) as a function of $M$ and $N$.\label{fig:stationary-bias}}
\end{figure}

The variance estimates show similar patterns to those of the bias and are shown in \cref{fig:stationary-variance} as the log standard deviation.
As suggested in the bias plots, the HMM and LN2 Stationary models have very low variance of their estimates.
The LN2 Trend model has much higher variance in estimates, particularly for short $N$ and long $M$.
\begin{figure}[ht]
  \includegraphics[width=\textwidth]{variance_stationary.pdf}
  \caption{The estimation variance for 1000 sequences generated by each of two stationary generating functions (row) and fit by each of three fitting functions (columns) as a function of $M$ and $N$.\label{fig:stationary-variance}}
\end{figure}

\subsection{Trend Case}

We next repeat the previous example but introduce a time trend in the data.
Thus the parameters of the NINO3 model are as \cref{tab:nino-stationary} except that $\gamma=0.015$ and the parameters of the Markov model are as \cref{tab:markov-stationary} except that $\gamma_1 = 0.015$.

For short $M$, particularly when $N$ is not too big, the stationary models (HMM and LN2 Stationary) perform well for both generative functions.
As $M$ increases, they begin to substantially underestimate the probability of future flooding.
As for the stationary case, when $N$ is short the LN2 Stationary model tends to over-estimate $\hat{p}_T$ because it sees variation caused by the NINO3 time series or the Markov state switching and assigns it to internal variability, leading to unrealistically fat tails in the estimate.
The trend model, by contrast, over-estimates $p_T$ except when $N$ is very large, mainly because the posterior uncertainty in the magnitude of the trend means that the model believes that a trend larger than observed is not unlikely.
\begin{figure}[ht]
  \includegraphics[width=\textwidth]{bias_trend.pdf}
  \caption{The estimation bias for 1000 sequences generated by each of two non-stationary generating functions (row) and fit by each of three fitting functions (columns) as a function of $M$ and $N$.\label{fig:trend-bias}}
\end{figure}

\begin{figure}[ht]
  \includegraphics[width=\textwidth]{variance_trend.pdf}
  \caption{The estimation variance for 1000 sequences generated by each of two non-stationary generating functions (row) and fit by each of three fitting functions (columns) as a function of $M$ and $N$.\label{fig:trend-variance}}
\end{figure}

\begin{figure}
  \includegraphics[width=\textwidth]{example_long.pdf}
  \caption{
    A single sequence of streamflow generated by the NINO3 function with (L) $\gamma=0$ and (R) $\gamma=0.015$. 
    The sequence is fit to each fit to three different fitting models (rows); the grey shading indicates the posterior 50\%, 80\%, and 99\% confidence intervals of $\hat{P}_T(t)$.
    Here $M=N=150$.\label{fig:example-long}
  }
\end{figure}

\begin{figure}
  \includegraphics[width=\textwidth]{example_short.pdf}
  \caption{
    As \cref{fig:example-short} except \(N=50\).\label{fig:example-short}
  }
\end{figure}

% -----------------------------------------------------------------------------
% CASE STUDY
% -----------------------------------------------------------------------------

\section{Portfolio Risk Across Space}

Many actors, particularly those in the private sector such as reinsurance vendors, may be exposed to concurrent risk in many different locations.
It is consequently of interest to examine how their risk portfolio risk may vary as a function of time.

We consider that the annual-maximum streamflow in each location depends only on the NINO 3.4 index as shown in \cref{eq:nino3} ($\gamma=0$ to enforce zero secular change).
However, we allow the dependance of streamflow at each location on the NINO 3.4 index to be a random variable, such that $\beta \sim \mathcal{N} \qty(0, 1)$.
This assumption represents observations that during El Ni\~no events drought occurs in some parts of the world and floods in others \citep{Ropelewski1987,Ward2014}.

\Cref{fig:spatial-risk} shows the proportion of locations that experience 100-year floods in each year, empirically defined from a simulated \SI{2000}{year} record at that location.
The orange line shows that if the \gls{iid} assumption is valid, the proportion of sites experiencing a 100-year flood each year is always between 0.005 and 0.015.
However, if the data is simulated from the log-normal distribution conditional on the NINO 3.4 series, as few as zero sites or as many as 20\% of all sites may experience floods in a particular year.
In other words, even though the portfolio has managed its risk by choosing locations with strongly positive, strongly negative, and near-zero dependence on \gls{enso}, the distribution of portfolio risk has a very fat tail, in agreement with previous analyses \citep{Bonnafous2017a}.
\begin{figure}
  \includegraphics[width=\textwidth]{case_study.pdf}
  \caption{
    Proportion of sites experiencing a 100-year flood return in each year.
    (a): A 250-year series of data simulated with the NINO 3.4 model (blue) and using an \gls{iid} model (orange). 
    (b): The probability distribution function of data simulated with the NINO 3.4 model and using an \gls{iid} model.
    This distribution is estimated using a \SI{2000}{year} record, of which the first \SI{250}{year} is shown in (a).\label{fig:spatial-risk}
  }
\end{figure}

% -----------------------------------------------------------------------------
% DISCUSSION
% -----------------------------------------------------------------------------

\section{Discussion}

Although the flood sequences shown in \cref{fig:example-long,fig:example-short,fig:stationary-sequences,fig:trend-sequences} exhibit the regime-like, quasi-periodic behavior observed in analysis of observational streamflow records, the assumption that the deterministic component of flood risk varies linearly with time and \gls{enso} is a simplification.
Reality, of course, is more complicated.
The probability of flooding in a particular location is modulated by multiple physical mechanisms, and dependence on a particular driver is rarely linear.
Further, the linear time dependance assumed is merely a placeholder for physical drivers, such as water storage capacity of the catchment's soil or the regional average specific humidity, that experience secular change.

More complex strategies also exist for estimating the risk of flood, or another natural hazard, from an observational record.
For example, analysis of flood frequency can be improved by combining information across space \citep{Merz2008a,Lima2016}, combining predictions from many models, or by through use of frequency- or wavelet-based methods \citep{Kwon2007}.
The approach being taken by many, including the \gls{ipcc}, is to consider discrete scenarios for what humans may do with respect to greenhouse gas emissions, and then attempt to use climate models to estimate the probability of extreme events conditional on occurrence of a particular scenario.
Because of inherent errors due to the discretization and parameterization of the equations of motion, as well as uncertainty about the underlying physical parameters, bias correction methods \citep[\eg quantile-quantile mapping;][]{Rajczak2016} are typically applied to the final outputs.
While this typically improves the model chain's representation of the historical record, this approach assumes a stationary relationship between modeled and observed rainfall at a point-by-point scale, an assumption that is unlikely to hold under the kinds of spatial and dynamical shifts anticipated under increased greenhouse gas scenarios such as expansion of the tropics and altered equator-pole temperature contrasts.\citep{Dankers2009,Ott2013,Merz2014,Dittes2017}.\footnote{Farnham et al 2018!}

From a decision making perspective, one could either design and build a project that gives us protection through the year 2100 considering both types of nonstationarities, or consider a sequence of projects every $M$ years that incrementally add (or not) protection.
In this context one could ask how the uncertainty (and hence risk) for these decisions depends on the design level, the project duration $M$, and the uncertainty associated with the estimation of $f(\hat{p}_T)$.
This study has considered only uncertainty associated with changing flood hazard, yet the large uncertainty associated with flood risk hazard and possible adaptation technologies would necessarily be relevant.



What is the implication for choices of p and M under different models for stationarity and uncertainty using some simple loss and cost  functions.
This consideration suggests that it may be more effective to understand the consequences for analysis focusing on shorter lived mitigation alternatives that may last for 1, 5, 10, 20, 30, or 50 years.
Thus, sequences of such smaller projects may in some cases better reflect climate adaptation goals, particularly given the possibilities that major climate change impacts in a particular place may occur far into the future; that climate mitigation efforts may be successful; or that the needs of human society may change drastically over these horizons.

The results presented in \cref{sec:results} illustrate sampling uncertainty and inter-model disagreement using several simple models for annual-maximum floods.
In reality, 

Infrastructure in the developed world is aging \citep{Ho2017}, leading to questions of whether and how to replace it.
In the developed world new infrastructure is being built.
Given a positive future discount rate and uncertainty associated with future urbanization, economic growth, and climate change, the expected net benefits of this infrastructure are highly uncertain.
At the same time the private sector has accepted an increasing share of global risk \citep{WorldEconomicForum2016}, which it has mitigated primarily through financial instruments.
How should different contracts be priced? When does it make sense to build large infrastructure?
These questions cannot be answered with existing methodologies.

How could  financial risk mitigation instruments be used in conjunction with structural design to identify a robust, adaptive climate risk mitigation strategy with sequential decisions that are informed by updated estimates of risk.


Since we know the true distribution of the flood sequences (we have generated them ourselves), using the log-normal for both generating and fitting enables interpretation of results as a lower bound on total uncertainty.

The idea that one needs to consider the joint uncertainty in the estimation and occurrence of climate extremes for the $N$ and $M$ periods for stationary conditions was originally considered by \citet{Lall1987}.
That analysis corresponds to what one would expect under \gls{iid} conditions, and would hence provide a baseline to compare the uncertainty distribution associated with a portfolio whose elements were designed using $N$ years of data with an application to $M$ future years at a common design probability of exceedance for the elements. 


% -----------------------------------------------------------------------------
% SUMMARY
% -----------------------------------------------------------------------------

\section{Summary}

The main point you are hitting is that the very nature of climate has long memory and some people point this out, many model it, and most future projections don't show this.
All corrections (even if we like them) address the marginal distribution.
So while the debate you refer to is interesting to a degree, the point we are making is that in terms of risk analysis it is not even tangentially relevant to reality and what we bring up -- our focus -- is strictly the risk based decision process and we are agnostic to the details of the models, other than the long memory features and spatial teleconnections associated with them that are seen as marks of the system behavior.
We actually demonstrate that the potential loss in a 2100 scenario has to be catastrophic and really out of the range of any historical variability for it to be relevant.
If we accept this proposition then we have a strong case for the mitigation of climate change but a much weaker case for adaptation, since if we are not able to mitigate that scenario then building a giant wall to control the 2100 scenario today will be economically and socially infeasible anyway.
So the next obvious question is in which situation would waiting and updating the decision make sense and this can be set up and explored or just discussed.

To really drive this paper home, we ought to end with a few recommendations for practitioners estimating and adapting to future climate risk such as:
\begin{itemize}
  \item For some combinations of $M$ and $N$, use of trend models leads to sub-optimal decisions \emph{even when there is a truely linear trend and the model is correctly specified}
  \item Consider the structure of the underlying trend, $N$, and $M$ when choosing a model
  \item Consider the bias-variance tradeoff of model complexity
\end{itemize}
We could even offer some recommendations as to when to choose ``insurance'' projects versus ``infrastructure'' projects, but this may just needlessly antagonize readers.

% -----------------------------------------------------------------------------
% END HERE
% -----------------------------------------------------------------------------

\appendix

\section{Thanks}

The authors thank Nandini Ramesh of Columbia University for providing the synthetic NINO3 index from a \SI{1000000}{year} run of the Cane-Zebiak model as described in \citet{Ramesh2017}.
The authors thank Columbia ITS for support running simulations on the cluster.
James thanks NSF.
Dave thanks grant.

\subsection{Are Generated Sequences Realistic?\label{sec:sequence-realistic}}

The models used to generate synthetic flood sequences used simple dynamics, but nevertheless were designed to incorporate regime-like and quasi-period variability at multiple timescales.

To verify that these synthetic streamflow sequences sufficiently represent these behaviors, we plot several sequences.
The ENSO model used a long run of the Cane-Zebiak model \citep{Zebiak1987,Ramesh2017} to determine \(\mu(t)\); a \SI{2500}{year} sub-set of the annualized time series is shown in \cref{fig:enso-ts}.
As described in the literature,\footnote{citations, and maybe some paleo stuff as well} this model shows quasi-periodic oscillations in the \SIrange{3}{7}{year} band but with substantial low-frequency modulation of extreme activity.
The index is also asymmetrically distributed, with a longer tail on the positive (El Ni\~{n}o) side than the negative (La Ni\~{n}a) phase, which is consistent with observations of ENSO.
Analysis of the power density spectrum shows a strong peak around 3-6 years.
\begin{figure}
  \includegraphics[width=\textwidth]{enso.pdf}
  \caption{
    (L): A \SI{1000}{year} sub-set of the synthetic time series described in \cref{sec:methods-nino}.
    (R): The power spectral density of the annualized time series. A moving window was applied to the density for smoothing.\label{fig:enso-ts}
  }
\end{figure}

\Cref{fig:stationary-sequences} shows 50 streamflow sequences generated with $M=50$ and $N=100$.
The ENSO sequences were generated with the parameters shown in \cref{tab:nino-stationary} Markov sequences were generated with the parameters shown in \cref{tab:markov-stationary}.
\begin{figure}[b]
  \includegraphics[width=\textwidth]{sequences_stationary.pdf}  
  \caption{
    50 streamflow sequences from (Top) the ENSO-based model described in \cref{sec:methods-nino} and (Bottom) the Markov chain modle described in \cref{sec:methods-markov}.
    Both models use a stationary parameterization $\gamma=0$.
    49 of the 50 sequences are plotted in gray, and one (randomly chosen) is plotted in blue for emphasis.
    Note that $y$-axis has a log scale.\label{fig:stationary-sequences}
  }
\end{figure}

To incorporate secular variability into our model, we repeat the exercise but add a trend term to the data.
For the ENSO time series, this mean setting \(\gamma=0.015\) while for the Markov case this meant a trend term on the wet state only, so that \(\gamma_1=0.015\).
The resulting sequences are shown in \cref{fig:trend-sequences}.
The trend has an extremely large impact on the probability of an extremely high flood, but the underlying time series structure is generally preserved.
\begin{figure}[b]d
  \includegraphics[width=\textwidth]{sequences_trend.pdf}
  \caption{
    As \cref{fig:stationary-sequences} but a trend term has been added by setting \(\gamma=0.015\).\label{fig:trend-sequences}
  }
\end{figure}

This choice of parameterizing secular change as a linear trend on the expected value of log streamflow was made to aid interpretability, and not because of any particular physical reasoning.
Indeed, for large $N$ this model suggests implausibly dry conditions and implausibly wet conditions for very large $M$.
However, given widespread use of trend-based models in the flood frequency analysis literature it is informative to consider the ``best-case'' scenario in which the model is correctly specified and there is a true, linear trend in the data.

\section{Notation}

\begin{description}
  \item[\( Q(t) \)] Annual-maximum flood series
  \item[\( T \)] Return period, in years
  \item[\( Q_T \)] Flood threshold
  \item[\( M \)] the project planning period
  \item[\( N \)] the length of the observational record, in years
  \item[\( x(t) \)] the annualized time series of the NINO3 index
  \item[\( p_T \)] the true probability that \( Q(t) \geq Q_T \) in the future $M$-year period
  \item[\( \hat{p}_T \)] the estimated probability that \( Q(t) \geq Q_T \) in the future $M$-year period
\end{description}

\printbibliography{}


\end{document}